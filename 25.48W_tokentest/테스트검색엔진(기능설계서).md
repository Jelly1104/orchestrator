- demo ver.001_251126 (구현기능요약)
  - 모델 : gpt-5-mini
  - 입력:
    1. 이미지만 → 이미지 캡션/속성 추출 후 요약
    2. 텍스트만 → 텍스트 요약
    3. 이미지+텍스트 → 둘 합쳐 요약
  - 출력: 최종 텍스트 요약 한 가지.
    ![image.png](attachment:7bbdc2bb-1670-4b09-b94d-f42aa3052332:image.png)
- demo ver.002_251127

  ## 1. 목표

  이미지 / 텍스트 / 혼합 입력을 받아
  **검색 성능과 추천 확장성까지 고려된 요약 2종을 자동 생성**하고,
  이를 기반으로 **BM25 + 벡터 검색을 결합한 Hybrid 엔진**을 구축한다.

  ***

  ```jsx
  사용자 Query/이미지 입력
            │
          Preprocess
     (토큰 제한/필터링)
            │
        Cache Lookup
     (재사용 가능 시 skip)
            │
           Embed
            │
        Hybrid Search
     (BM25 + Vector 조합)
            │
           Ranking
            │
         최종 결과 출력
  ```

  ## 2. 전체 구조 (High-level)

  ### 2.1 입력 → 요약 → 임베딩 → 검색

  ![simple flow](attachment:ab384803-d56a-4d53-b5c2-fbfc22552528:image.png)
  simple flow
  | 입력 | 처리 | 출력 |
  | --------------- | ----------------------- | -------------------------------- |
  | 이미지만 | 캡션 + 속성 추출 → 요약 | Lexical Summary / Vector Summary |
  | 텍스트만 | 의미 요약 | Lexical Summary / Vector Summary |
  | 이미지 + 텍스트 | 결합 분석 → 요약 | Lexical Summary / Vector Summary |

  ***

  ## 3. 요약 생성 (Core Logic)

  ### 3.1 2종 요약 구조

  | 요약 종류           | 목적        | 작성 규칙                        |
  | ------------------- | ----------- | -------------------------------- |
  | **Lexical Summary** | BM25 검색용 | 원문 key terms 최대한 보존       |
  | **Vector Summary**  | 벡터 검색용 | 의미 중심 재구성(단어 변경 허용) |

  > 예시
  >
  > 원문: "대치동 피부과 양도, 월세400, 보증금 5000, 역세권"
  >
  > Lexical → `대치동 피부과 양도, 월세400, 보증금5000`
  >
  > Vector → `대치동 피부과 양도 매물, 역세권 위치, 조건 안정적`

  ***

  ## **4. 토큰 관리 설계 (필수)**

  > LLM 요약은 말 잘 안 듣는 경우가 있음 → **요약 후 토큰 수를 직접 검증**해야 한다.

  ```tsx
  if tokens(original) > limit:
      summary1 = summarize(original)
      if tokens(summary1) > limit:
          summary2 = summarize(summary1, aggressive=true)
  use summary2
  ```

  - 임베딩 모델의 최대 입력 토큰량을 넘기지 않기 위해
  - 비용 제어
  - 실시간 요청에서 지연이 발생하지 않도록

  ***

  ## **5. 실시간 임베딩 설계 (검색엔진의 핵심 안정장치)**

  ![sequence ](attachment:df2e3f9d-2ca8-4630-9e1b-4c253db430eb:image.png)
  sequence

  ***

  ### 5.1 문서 vs 쿼리 처리 전략

  | 구분                   | 처리 방식                                  |
  | ---------------------- | ------------------------------------------ |
  | **문서 요약/임베딩**   | 오프라인(batch), 안정적 환경에서 사전 계산 |
  | **사용자 쿼리 임베딩** | 실시간 → 성능·비용 리스크 존재             |

  따라서 실시간 요청에 대한 안전장치 필요.

  ***

  ### 5.2 실시간 임베딩 Guardrails (반드시 필요)

  ### (1) 토큰 폭발 방지

  | 규칙                            | 이유               |
  | ------------------------------- | ------------------ |
  | 쿼리 길이 128~256 토큰으로 절단 | 지연·비용 제어     |
  | 임베딩 모델은 경량 모델 사용    | 실시간 처리에 유리 |
  | LLM 임베딩 절대 금지            | 비용 폭발          |

  ***

  ### (2) 임베딩 캐싱 (Critical)

  반복되는 쿼리는 임베딩 재사용.

  - `query_hash → embedding_vector`
  - Redis/KV에 저장
  - TTL + LRU 만료 적용
    > 현실에서는 80% 이상이 캐시 히트로 조용히 처리됨.

  ***

  ### (3) 이미지 쿼리 Rate Control

  | 정책              | 이유                       |
  | ----------------- | -------------------------- |
  | 이미지 리사이징   | 불필요한 compute 낭비 제거 |
  | 업로드 rate-limit | 남용 방지                  |
  | 이미지 해시 캐시  | 동일 이미지 재처리 금지    |

  ***

  ### (4) 폴백 전략

  임베딩 생성 느릴 경우:
  → **Vector Search Skip → BM25만 실행**
  검색서비스는 “빠르게 유의미한 결과”가 더 중요.

  ***

  ## 6. Hybrid 검색 구조

  ```
  Query → Preprocess → Cache Lookup
        → (미스) → Embed → Cache Save

  → Vector Search (top-k)
  → BM25 Search (top-k)

  → Hybrid Ranking → Result
  ```

  - mermaid code

    ```mermaid
    sequenceDiagram
        participant U as User
        participant A as API Server
        participant C as Cache
        participant E as Embedding Model
        participant V as Vector DB
        participant B as BM25 Index
        participant R as Ranker

        U->>A: Search request (text or image)
        A->>C: Check query embedding in cache
        C-->>A: Hit or Miss

        alt Cache Miss
            A->>E: Generate embedding from query
            E-->>A: Return embedding vector
            A->>C: Store embedding in cache
        end

        A->>V: Vector search
        A->>B: BM25 lexical search

        V-->>A: vector results
        B-->>A: lexical results

        A->>R: Hybrid ranking (Merge + Re-rank)
        R-->>A: Final results

        A-->>U: Response
    ```

  ***

  ## 7. 향후 확장: 유사 문서 추천 (Recommendation Ready 구조)

  검색엔진 설계가 커지면 필수적으로 붙는 기능.

  ### 7.1 item-to-item 구조

  문서 D를 기준으로:

  ```css
  D.embedding → Vector DB top-k → 후보 관련 문서
  → 필터링 → Ranking → related list
  ```

  | 항목                       | 설명                         |
  | -------------------------- | ---------------------------- |
  | `embedding_model_version`  | 모델 교체에 대비한 버전 관리 |
  | `summary_version`          | 요약 프롬프트 변경 대비      |
  | `related_items` 테이블     | 추천 결과 저장 공간 확보     |
  | 검색 응답에 `related` 필드 | 지금부터 자리 잡아두기       |

  ### 7.3 Ranking 모듈 분리

  검색용: `rankSearchResults()`
  추천용: `rankRelatedItems()`
  → 책임 분리해두면 나중에 유지보수 지옥을 피함.

  ***

  ## 8. 전체 Diagram (Develop ver.)

  ### 8.1 Full Sequence (Updated)

  ```mermaid
  sequenceDiagram
      participant U as User
      participant A as API Server
      participant C as Cache
      participant LLM as Summary Model
      participant EMB as Embedding Model
      participant VDB as Vector DB
      participant BM as BM25 Index
      participant RK as Ranker

      %% Input Processing
      U->>A: Upload text/image/mixed
      A->>LLM: Generate Lexical/Vector Summary
      LLM-->>A: Summaries

      A->>A: Token Check (tiktoken)
      alt Over Limit
          A->>LLM: Reduce Summary (aggressive)
          LLM-->>A: Reduced Summaries
      end

      A->>EMB: Embed Vector Summary
      EMB-->>A: embedding_vector

      %% Store Document Embedding (offline or async)
      A->>VDB: Insert document vector

      %% Query Flow
      U->>A: Search Query
      A->>C: Check cached embedding
      C-->>A: Hit/Miss

      alt Miss
          A->>EMB: Embed query
          EMB-->>A: query_vector
          A->>C: Save cache
      end

      A->>VDB: Vector top-k
      A->>BM: BM25 top-k

      VDB-->>A: vector results
      BM-->>A: lexical results

      A->>RK: Hybrid ranking
      RK-->>A: Final results

      A-->>U: Search Response (+ optional related)

  ```

  ***

  ## 9. 개발 시작 전 최종 체크리스트

  ## 요약·임베딩·검색 레이어

  - [x] Lexical Summary / Vector Summary 구조 확정
  - [x] Token limit → summarize → re-check
  - [x] Embedding Wrapper: caching + timeout + fallback
  - [x] BM25 + Vector Search 병합 로직
  - [x] Ranking 모듈 분리

  ## 스키마/버전관리

  - [ ] `content_id`, `summary_version`, `embedding_model_version`
  - [ ] `related_items` 테이블 생성
  - [ ] 문서/쿼리 별 임베딩 저장/로그 구조 확정

  ## 추천 확장 대비

  - [ ] API Response에서 `related` 필드 자리 확보
  - [ ] item-to-item 계산 공간 남겨두기
  - [ ] Ranking 모듈 2계층 분리(Search/Recommend)

  - 가드레일

    # 🔒 _Search Embedding Guardrails (운영 안전장치)_

    ## 1. 핵심 원칙

    | 구분        | 전략                                    |
    | ----------- | --------------------------------------- |
    | 문서 임베딩 | **오프라인·배치 처리** → DB에 벡터 저장 |
    | 쿼리 임베딩 | **실시간 생성 1회** → 부담 최소화 필요  |
    | 이미지 쿼리 | Caption → Text Embedding → 검색         |

    실시간 리소스는 **문서가 아니라 쿼리**가 소비한다.
    따라서 쿼리를 안전하게 다룰 설계가 필요하다.

    ***

    ## 2. 실시간 쿼리 처리에서 반드시 들어가야 할 제한 조치(Guardrails)

    ### 2-1. ⚡ 지연 & 토큰 폭발 방지

    | 항목             | 규칙                                                     |
    | ---------------- | -------------------------------------------------------- |
    | 쿼리 길이 제한   | 150~256 토큰 컷, 길면 요약/자름                          |
    | 임베딩 모델 선택 | LLM(gpt)급 X → **경량 embedding 모델 사용**              |
    | 타임아웃 정책    | 임베딩 지연 발생 시 → **Vector 검색 생략 & BM25만 진행** |

    > “느려도 기다린다” 금지. → 검색 서비스는 응답속도 > 완벽함

    ***

    ### 2-2. 🧠 캐싱 필수 (Critical)

    **동일 쿼리 = 다시 임베딩하지 않는다**
    | 전략 | 효과 |
    | --------------------------------------- | -------------------------- |
    | `query_string → hash → embedding cache` | 동일 입력 반복 → 비용 절감 |
    | 캐시 미스 시에만 임베딩 호출 | 실시간 계산 최소화 |
    | LRU / TTL 기반 만료 적용 | 캐시 팽창 방지 |

    > 실제 운영에서는 **“임베딩 거의 한 번만 하고 재사용한다”**가 현실적 형태.

    ***

    ### 2-3. 🖼 이미지 쿼리 강화정책

    텍스트보다 처리가 무거우므로 제한 필요.
    | 정책 | 이유 |
    | -------------------------------- | ----------------------- |
    | 이미지 해상도/용량 제한 → Resize | 계산 비용 폭발 방지 |
    | 1유저 업로드 rate-limit | 남용 방지 |
    | 이미지도 해시캐싱 | 동일 이미지 재처리 금지 |
    이미지 검색은 **부가 기능**, Text Search가 **메인**

    ***

    ### 2-4. 💰 비용 추적 가능하게 설계

    모델 변경 대비 + 운영비 계산 가능해야 함.
    로그 필드 예시:

    ```
    user_id
    query_id
    token_count
    model_name
    timestamp

    ```

    Config에 유지할 항목:

    ```
    embedding_model_name
    max_tokens
    encoding
    cost_per_1k_tokens

    ```

    → 나중에 단가 바뀌면 **즉시 비용 시뮬레이션 가능**

    ***

    ## 3. 실시간 임베딩 운영 설계 정리 (One View)

    | 대상        | 처리 전략                                       |
    | ----------- | ----------------------------------------------- |
    | 문서        | OFFLINE — 배치 임베딩 / DB 저장                 |
    | 텍스트 쿼리 | 실시간 임베딩(캐시 우선) / 길이 제한 / 타임아웃 |
    | 이미지 쿼리 | Caption → Embedding → 캐시 / 사이즈·빈도 제한   |
    | 장애 상황   | 폴백: Vector Skip → BM25 단독 검색              |

    > 이 정도면 QPS 증가에도 안전하게 버틴다.

    ***

    ## 4. 코딩 시 구현 팩터 (핵심 한 줄)

    ```tsx
    embedQuery() = caching + tokenLimit + timeout + fallback(BM25 - only);
    ```

    상위 검색 레이어는 **임베딩 동작을 신경쓸 필요 없게 만든다.**

    ***

    필요하면 나는 **이걸 Figma/시퀀스에 추가한 Extended Diagram 버전까지 생성 가능**하고,
    그 다음은 네 말대로 바로 _코드 작성 단계 들어가면 된다._

- demo ver.002*251127*구현기능요약
  ![image.png](attachment:ce45e544-8e01-4a40-b0f2-8a5853ee6d6b:image.png)
  - ver02 – 현재 구현된 기능 요약
    - 텍스트/이미지 입력 요약 기능 (Summaries API /summarize)
      - 이미지 전처리(PNG→RGB 변환), 텍스트/이미지 결합 요약, gpt-5-mini 기반 출력
      - 누락 입력 시 422 응답 처리
    - 검색 기능(Search API /search)
      - 토큰 제한(enforce_token_limit) 적용
      - 쿼리 임베딩 캐시(get_query_embedding)
      - BM25 흉내 결과 + Vector 기반 결과 → hybrid_merge로 가중 병합
      - 프론트에서 검색 입력→결과 리스트 렌더링
    - [전체 모듈 TDD (18개) 통과](https://www.notion.so/TDD-2b8879603bef80d6b1e1f2c55fec825c?pvs=21)
      - M01~M05(요약·임베딩·하이브리드)
      - API01/02 동작 확인 완료
    - 프론트 통합 데모
      - 이미지 요약 + 검색 UI 모두 정상 작동
- demo ver.003_251128

  # 📘 **기능설계서 ver.003**

  ### _Multi-Model Summary · Token Debug · Hybrid Search 확장 버전_

  ***

  # 1. 목표

  이미지 / 텍스트 / 혼합 입력을 받아
  **Lexical Summary + Vector Summary를 다중 모델 기반으로 생성**하고,
  요약 과정 전체(토큰 수, 축소 단계)를 **사용자에게 투명하게 디버깅 가능하도록 제공**한다.
  또한,
  사용자가 직접 **요약 모델을 선택하고**
  모델별 **token limit을 입력·조절**할 수 있도록 UI/백엔드를 확장한다.
  해당 요약들을 기반으로 기존 **BM25 + Vector Hybrid Search**를 유지하며
  검색 품질과 추천 확장성을 향상시키는 것을 목표로 한다.

  ***

  # 2. 전체 구조 (High-level)

  ### 2.1 입력 → 요약 → 임베딩 → 검색 (ver.002 동일 구조)

  | 입력            | 처리                        | 출력                             |
  | --------------- | --------------------------- | -------------------------------- |
  | 이미지만        | 캡션 + 속성 추출 → 2종 요약 | Lexical Summary / Vector Summary |
  | 텍스트만        | 의미 분석 → 2종 요약        | Lexical Summary / Vector Summary |
  | 이미지 + 텍스트 | 결합 분석 → 2종 요약        | Lexical Summary / Vector Summary |

  ***

  # 3. 2종 요약 생성 (Core Logic) — ver.003 확장

  ### 3.1 2종 요약 구조 (ver.002 동일)

  | 요약 종류           | 목적                          | 작성 규칙                |
  | ------------------- | ----------------------------- | ------------------------ |
  | **Lexical Summary** | BM25 검색 성능 강화           | 원문 핵심 단어 최대 보존 |
  | **Vector Summary**  | 벡터 검색·의미 기반 검색 강화 | 표현/어휘 변경 허용      |

  ***

  ### 3.2 **(ver.003 신규) 멀티 모델 기반 구조**

  각 Summary는 “단일 LLM”이 아니라,
  사용자가 선택하는 **Lexical Model / Vector Model 별도 모델**에서 생성된다.
  프론트에서 제공해야 할 항목:

  - Lexical Summary 모델 드롭다운
  - Vector Summary 모델 드롭다운
    백엔드 구조:

  ```json
  {
    "lexical_v1": { "min_limit": 64, "max_limit": 512, "default_limit": 256 },
    "vector_v1": { "min_limit": 64, "max_limit": 1024, "default_limit": 512 }
  }
  ```

  ***

  # 4. 토큰 관리 설계 (ver.002 확장)

  ver.002의 “토큰 수 검증 → 재요약” 구조는 그대로 유지하되, ver.003에서는 아래가 확장된다:

  ### 4.1 사용자 입력 token limit (프론트 textbox)

  - lexical_limit
  - vector_limit
  - 모두 프론트 textbox로 입력
  - 모델별 min/max Limit을 프론트 안내창으로 표시

  ### 4.2 백엔드 Token Limit Guard

  사용자 입력은 반드시 모델별 범위로 clamp:

  ```
  limit = max(min_limit, min(max_limit, user_input))

  ```

  ### 4.3 (ver.003 신규) multi-step summary 디버그

  각 요약은 다음 형태로 단계별로 기록됨:

  ```json
  {
    "steps": [
      { "summary": "1차 요약", "tokens": 320 },
      { "summary": "2차 요약", "tokens": 180 },
      { "summary": "3차 요약", "tokens": 90 }
    ],
    "final_summary": "최종 요약",
    "final_tokens": 90
  }
  ```

  Lexical / Vector 모두 제공.

  ***

  # 5. 실시간 임베딩 Guardrails (ver.002 유지)

  기존 ver.002와 동일.

  - 토큰 절단
  - 경량 embedding 사용
  - 캐싱
  - 이미지 업로드 rate-limit
  - Vector 실패 시 BM25-only fallback
    (이 파트는 ver.002 그대로 유지)

  ***

  # 6. Hybrid 검색 구조 (ver.002 유지)

  Hybrid 결과는 변하지 않으나,
  **검색 결과 위에 디버그 정보(debug panel)**가 추가됨
  ![image.png](attachment:bcfb7c1d-0bff-4836-8428-b7602bdc1af2:image.png)

  - mermaid

    ```mermaid
    sequenceDiagram
        participant U as User
        participant A as API Server
        participant M as Summary Models
        participant T as Token Utils
        participant E as Embed
        participant V as Vector DB
        participant B as BM25
        participant R as Ranker

        %% Summary Phase
        U->>A: text/image input + model selections + limit values
        A->>M: Lexical summary (model_id)
        M-->>A: summary1

        A->>T: Token check + multi-step reduce
        T-->>A: lexical_steps

        A->>M: Vector summary (model_id)
        M-->>A: summary1

        A->>T: Token check + multi-step reduce
        T-->>A: vector_steps

        %% Embedding & Search
        A->>E: embed(vector_summary)
        E-->>A: embedding_vector

        A->>V: vector top-k
        A->>B: bm25 top-k

        V-->>A: vec_res
        B-->>A: lex_res

        A->>R: merge+rank
        R-->>A: final_results

        A-->>U: final_results + debug(lexical/vector/input)

    ```

  ***

  # 7. (ver.003 신규) 디버그 패널 구조

  검색 결과 위에 표시해야 할 정보:

  ### 1) Input Token 수

  ### 2) Lexical Summary

  - final summary
  - token
  - step-by-step token 변화

  ### 3) Vector Summary

  - final summary
  - token
  - step-by-step token 변화
    프론트 예시:

  ```
  [입력 텍스트 토큰: 350]

  [Lexical Summary]
  1차(320): ...
  2차(180): ...
  3차(90): ...
  최종 요약: ...

  [Vector Summary]
  1차(330): ...
  2차(200): ...
  3차(70): ...
  최종 요약: ...

  ```

  ***

  # 8. 전체 Sequence (ver.003 버전)

  ```mermaid
  sequenceDiagram
      participant U as User
      participant A as API Server
      participant M as Summary Models
      participant T as Token Utils
      participant E as Embed
      participant V as Vector DB
      participant B as BM25
      participant R as Ranker

      %% Summary Phase
      U->>A: text/image input + model selections + limit values
      A->>M: Lexical summary (model_id)
      M-->>A: summary1

      A->>T: Token check + multi-step reduce
      T-->>A: lexical_steps

      A->>M: Vector summary (model_id)
      M-->>A: summary1

      A->>T: Token check + multi-step reduce
      T-->>A: vector_steps

      %% Embedding & Search
      A->>E: embed(vector_summary)
      E-->>A: embedding_vector

      A->>V: vector top-k
      A->>B: bm25 top-k

      V-->>A: vec_res
      B-->>A: lex_res

      A->>R: merge+rank
      R-->>A: final_results

      A-->>U: final_results + debug(lexical/vector/input)

  ```

  ***

  # 9. ver.003 상세 요구사항 체크리스트

  ### 요약/토큰 (필수)

  - [ ] multi-model 기반 lexical summary
  - [ ] multi-model 기반 vector summary
  - [ ] token limit 사용자 입력
  - [ ] 모델별 min/max limit 안내
  - [ ] multi-step summary 기록(debug)

  ### 검색 (필수)

  - [ ] 기존 Hybrid Search 유지
  - [ ] summary debug panel 추가

  ### API (필수)

  - [ ] `/summary-models` — 모델/limit 정보 제공
  - [ ] `/search` — ver.003 확장된 payload/response

  ### 프론트 (필수)

  - [ ] lexical/vector 모델 선택 드롭다운
  - [ ] token limit textbox + 안내 문구
  - [ ] debug panel 렌더링

  ***

  # 10. 향후 확장 (ver.002 → ver.003 자연스러운 이어짐)

  - 요약 품질 스코어링
  - 모델별 학습/버전관리
  - GPT→OpenSearch 파이프라인 최적화
  - 문서 기반 추천(item-to-item)

  ***

  # ✔ ver.002와 동일한 구성/톤으로 재정리 끝

  - 구조 동일함
  - ver.003 신규 기능은 ver.002 목차 안에 자연스럽게 삽입됨
  - 문단 스타일, 설명 방식, 표 구성 모두 ver.002에 맞춤
    원하면 **Figma 다이어그램 ver.003용**도 그려줄 수 있음.

  ### 나의 말:

  Figma 다이어그램 ver.003용와 mermaid용 시퀀스 ver.003도 부탁해

  ### ChatGPT의 말:

  [Figma](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzQxMzVfNDI5NSkiPgo8cGF0aCBkPSJNOC4xNjY2NyAyMkMxMC4wMDY3IDIyIDExLjUgMjAuNTA2NyAxMS41IDE4LjY2NjdWMTUuMzMzM0g4LjE2NjY3QzYuMzI2NjcgMTUuMzMzMyA0LjgzMzMzIDE2LjgyNjcgNC44MzMzMyAxOC42NjY3QzQuODMzMzMgMjAuNTA2NyA2LjMyNjY3IDIyIDguMTY2NjcgMjJaIiBmaWxsPSIjMEFDRjgzIi8+CjxwYXRoIGQ9Ik00LjgzMzMzIDEyQzQuODMzMzMgMTAuMTYgNi4zMjY2NyA4LjY2NjY3IDguMTY2NjcgOC42NjY2N0gxMS41VjE1LjMzMzNIOC4xNjY2N0M2LjMyNjY3IDE1LjMzMzMgNC44MzMzMyAxMy44NCA0LjgzMzMzIDEyWiIgZmlsbD0iI0EyNTlGRiIvPgo8cGF0aCBkPSJNNC44MzMzMyA1LjMzMzMzQzQuODMzMzMgMy40OTMzMyA2LjMyNjY3IDIgOC4xNjY2NyAySDExLjVWOC42NjY2N0g4LjE2NjY3QzYuMzI2NjcgOC42NjY2NyA0LjgzMzMzIDcuMTczMzMgNC44MzMzMyA1LjMzMzMzWiIgZmlsbD0iI0YyNEUxRSIvPgo8cGF0aCBkPSJNMTEuNSAySDE0LjgzMzNDMTYuNjczMyAyIDE4LjE2NjcgMy40OTMzMyAxOC4xNjY3IDUuMzMzMzNDMTguMTY2NyA3LjE3MzMzIDE2LjY3MzMgOC42NjY2NyAxNC44MzMzIDguNjY2NjdIMTEuNVYyWiIgZmlsbD0iI0ZGNzI2MiIvPgo8cGF0aCBkPSJNMTguMTY2NyAxMkMxOC4xNjY3IDEzLjg0IDE2LjY3MzMgMTUuMzMzMyAxNC44MzMzIDE1LjMzMzNDMTIuOTkzMyAxNS4zMzMzIDExLjUgMTMuODQgMTEuNSAxMkMxMS41IDEwLjE2IDEyLjk5MzMgOC42NjY2NyAxNC44MzMzIDguNjY2NjdDMTYuNjczMyA4LjY2NjY3IDE4LjE2NjcgMTAuMTYgMTguMTY2NyAxMloiIGZpbGw9IiMxQUJDRkUiLz4KPC9nPgo8ZGVmcz4KPGNsaXBQYXRoIGlkPSJjbGlwMF80MTM1XzQyOTUiPgo8cmVjdCB3aWR0aD0iMTMuMzM2IiBoZWlnaHQ9IjIwIiBmaWxsPSJ3aGl0ZSIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoNC44MzIgMikiLz4KPC9jbGlwUGF0aD4KPC9kZWZzPgo8L3N2Zz4K)
  Figma
  아래는 **Figma 다이어그램(ver.003)**은 이미 상단에 생성해두었고,
  이제 **mermaid 시퀀스 다이어그램(ver.003)** 그대로 제공한다.

  # 💡 지금 준비된 것 요약

  ### ✔ ver.003 Figma 다이어그램

  → 이미 위에 자동 렌더링됨
  → Multi-Model Summary → Multi-step token → Hybrid Search 전체 흐름 완성

  ### ✔ ver.003 Mermaid Sequence Diagram

  → 위 코드 그대로 사용 가능
  필요하면 ver03의 **데이터모델(Entity 구조)** 또는
  **API 스펙 표준화 버전**까지도 뽑아줄 수 있다.

- demo ver.003*251128*구현기능요약
  ✔“임시 테스트용 shorten 함수”
  ![image.png](attachment:79fee2a7-2ade-4c48-8ef7-a813e31b7b89:image.png)
- demo ver.003.1_251201

  ## 1) `content_summary` 스키마 문서화

  노션에 그대로 붙여 쓰라고 만든 버전이다.

  ```
  [Dataset] content_summary_100_samples

  필수 필드
  - content_id: string | int
    - 문서 고유 ID (DB PK와 매핑 가능)
  - title: string
    - 화면에 노출되는 한 줄 제목
  - content_summary: string
    - 현재 운영/도메인에서 사용하는 요약 텍스트
    - ver04 평가 시, "입력 원문"으로 사용

  향후 ver04 이후에 추가 예정 필드 (출력/평가 결과 저장용)
  - lexical_summary_v03: string        # ver03 파이프라인으로 생성된 lexical 요약
  - vector_summary_v03: string         # ver03 파이프라인으로 생성된 vector 요약
  - lexical_tokens_v03: int            # lexical_summary_v03 토큰 수
  - vector_tokens_v03: int             # vector_summary_v03 토큰 수
  - orig_tokens_v03: int               # content_summary 토큰 수
  - lexical_keyword_coverage_v03: float  # lexical 키워드 보존율(0~1)
  - model_id_lexical_v03: string       # 사용 모델/프롬프트 버전
  - model_id_vector_v03: string
  - summary_version: string            # 예: "v03", "v04"

  ```

  예시 레코드:

  ```json
  {
    "content_id": 375344,
    "title": "청주시 흥덕구 일반의원(가정의학과·내과) 양도",
    "content_summary": "충북 청주시 흥덕구 복대동에 위치한 2005년 개원 가정의학과+내과 의원 ... (생략)"
  }
  ```

  이 정도면 “스키마는 정해졌다”로 치면 된다.

  ***

  ## 2) `tiktoken` 설치 + `token_count_tiktoken()` 설계

  ### 2-1. 의존성 추가

  `backend/requirements.txt` (또는 poetry/uv 등 쓰는 방식에 맞게):

  ```
  tiktoken>=0.7.0

  ```

  설치:

  ```bash
  cd claude/claude/backend
  . .venv/bin/activate   # 쓰는 가상환경대로
  pip install tiktoken

  ```

  ### 2-2. 토큰 카운트 유틸 함수

  `app/core/token_utils.py` 안에 추가:

  ```python
  import tiktoken

  # 전역으로 encoder 하나만 잡아둔다.
  _ENCODER = tiktoken.get_encoding("o200k_base")
  # 나중에 gpt-5-mini가 별도 이름 생기면 encoding_for_model(...)로 교체 가능

  def token_count_tiktoken(text: str) -> int:
      """tiktoken(o200k_base) 기준 실제 토큰 수 계산."""
      if text is None:
          return 0
      return len(_ENCODER.encode(text))

  ```

  기존의 naive `token_count`는 그대로 두고:

  - ver03 검색 디버깅용 UI: 계속 naive 써도 됨
  - ver04 “정식 평가용”: `token_count_tiktoken` 사용
    이렇게 역할 분리해 두는 게 안정적이다.

  ***

  ## 3) lexical / vector 토큰 limit 값 결정 기준

  `content_summary_100_samples.json`을 대충 whitespace 기준으로 돌려보면:

  - 최소 토큰 수: **62**
  - 중앙값(median): **188**
  - 90퍼센타일(p90): **291**
  - 최대: **937**
    (아주 긴 꼬리가 몇 개 있다)
    이 숫자 감으로 limit를 잡으면 된다.

  ### 추천 초깃값 (ver04 실험용)

  ```
  Lexical용:
  - min_limit: 64
  - default_limit: 192   # median(188) 근처
  - max_limit: 512

  Vector용:
  - min_limit: 64
  - default_limit: 128   # lexical보다 더 세게 압축
  - max_limit: 512

  ```

  그래서 `SUMMARY_MODELS`는 이렇게 손보면 된다:

  ```python
  SUMMARY_MODELS = {
      "lexical_v1": {
          "id": "lexical_v1",
          "label": "Lexical v1 (키워드 유지)",
          "kind": "lexical",
          "min_limit": 64,
          "max_limit": 512,
          "default_limit": 192,
      },
      "vector_v1": {
          "id": "vector_v1",
          "label": "Vector v1 (의미 압축)",
          "kind": "vector",
          "min_limit": 64,
          "max_limit": 512,
          "default_limit": 128,
      },
  }

  ```

  실제 ver04 평가에서는:

  - `token_count_tiktoken(content_summary)` 분포를 한 번 더 보고
  - 위 숫자를 미세 조정하면 된다. 지금은 “테스트 시작용 reasonable default” 수준.

  ***

  ## 4) “키워드 보존” 최소 룰 정의

  두환 요구사항 그대로 코드로 옮기기 좋은 수준으로 정리하면:

  ### 4-1. 키워드 후보 추출 규칙 (간단 버전)

  1. 원문과 lexical 요약 둘 다:
     - 공백 기준 토큰화
     - 1글자짜리 토큰 제거
     - 숫자·단위(평, 층, 호, 억, 만, 원 등) 포함 토큰은 그대로 유지
     - 일반 명사/고유명사 비슷한 토큰만 남기기 (POS 태깅은 나중 단계)
  2. “키워드 리스트”:

  ```
  keywords_orig   = 위 규칙으로 뽑은 원문 키워드 집합
  keywords_lex    = 위 규칙으로 뽑은 lexical 요약 키워드 집합

  ```

  ### 4-2. 보존율 정의

  ```
  coverage = |keywords_orig ∩ keywords_lex| / max(1, |keywords_orig|)

  ```

  ### 4-3. 최소 기준 (ver04 테스트용)

  ```
  - coverage >= 0.6 이상이면 Pass
  - coverage < 0.6 인 경우, "키워드 손실 가능성 있음"으로 플래그 처리
  - 추가로 아래 슬롯이 빠지면 무조건 경고:
    - 지역(동/구 단위 지명)
    - 진료과/업종
    - 핵심 숫자 정보(보증금, 월세, 권리금 등)

  ```

  정말 귀찮으면 1차 버전은 “coverage만 보고” 하고,
  슬롯 체크는 사람이 실패 케이스 위주로 눈으로 확인해도 된다.

  ***

  ## 5) 결과 리포트 템플릿 정의

  나중에 파이썬에서 CSV/Markdown으로 뽑을 틀을 미리 고정해두는 단계.

  ### 5-1. per-document 표 형식

  ```markdown
  | content_id | title                                   | orig_tok | lex_tok | vec_tok | lex_cov | flags             |
  | ---------: | --------------------------------------- | -------: | ------: | ------: | ------: | ----------------- |
  |     375344 | 청주시 흥덕구 일반의원(가정의학과) 양도 |      220 |     150 |     110 |    0.82 | OK                |
  |     375345 | 대치동 피부과 양도                      |      260 |     180 |     120 |    0.55 | LOW_COVERAGE_AREA |
  ```

  필드 의미:

  - `orig_tok` : `content_summary` 토큰 수 (tiktoken 기준)
  - `lex_tok` / `vec_tok`: 각 최종 요약 토큰 수
  - `lex_cov` : 위에서 정의한 coverage
  - `flags` : 자동/수동 코멘트 (예: `MISSING_PRICE`, `LOW_COVERAGE`, `OK` 등)

  ### 5-2. 상단 요약 블록

  리포트 문서 맨 위에 붙일 “한 눈 요약”:

  ```markdown
  - 샘플 수: 100
  - 토큰 감소율
    - Lexical: 평균 230 → 170 (약 26% 감소)
    - Vector : 평균 230 → 130 (약 43% 감소)
  - Lexical 키워드 보존율
    - 평균: 0.80
    - 최소: 0.55 (6건)
  - 주요 이슈 패턴
    - 금액/보증금 누락: 3건
    - 지역(동 단위) 누락: 2건
    - 진료과/업종 포지셔닝 애매: 1건
  ```

  ver04에서는:

  - 평가 스크립트가 이 표와 요약 블록을 자동으로 만들어주고,
  - 너는 그대로 노션에 복붙하거나 CSV를 엑셀로 열어서 보면 된다.

- Demo ver.004_251201

  # 📘 기능설계서 ver.004

  ### _content_summary_100_samples 배치 평가 · 요약 품질 리포트_

  ***

  ## 1. 목표

  - ver03에서 만든 **Lexical / Vector Summary + 토큰 디버그 파이프라인**을 사용해
    `content_summary_100_samples.json` 전체에 대해 **배치 평가**를 수행한다.
  - 각 문서에 대해:
    - 원문 토큰 수 / 요약 토큰 수 (tiktoken 기준)
    - 토큰 감소율
    - Lexical 키워드 보존율
  - 위 결과를 **리포트 파일(표 형식)**로 생성해,
    검색엔진 v01 적용 전 “요약 품질”을 정량·정성 검토할 수 있게 하는 것이 목적.
    온라인 API 기능 추가가 아니라,
    **오프라인·단발성 평가 파이프라인**이라는 걸 전제로 한다.

  ***

  ## 2. 범위

  ### 2.1 포함

  - 입력: `content_summary_100_samples.json`
  - 출력:
    - per-document 평가 결과 테이블 (CSV/Markdown 둘 중 하나)
    - 전체 통계 요약 블록
  - 사용 파이프라인:
    - ver03의 Lexical/Vector 요약 로직 (동일 프롬프트/설정)
    - ver04의 `token_count_tiktoken()` (o200k_base)
  - 평가 지표:
    - 토큰 감소율
    - Lexical 키워드 보존율

  ### 2.2 제외 (범위 밖)

  - OpenSearch 색인/검색
  - 온라인 API 변경 (`/search`, `/summarize` 스펙 변경 없음)
  - 추천(item-to-item) 계산
  - UI 변경 (리포트는 파일로만 생성, 프론트 연동은 나중에)

  ***

  ## 3. 입력/출력 정의

  ### 3.1 입력 데이터

  - 파일: `content_summary_100_samples.json`
  - 스키마:
    | 필드명 | 타입 | 설명 |
    | --------------- | ------------ | ----------------------------------------- |
    | content_id | string / int | 문서 고유 ID |
    | title | string | 한 줄 제목 |
    | content_summary | string | 도메인에서 쓰이는 요약 텍스트 (입력 원문) |
    필수: 위 3개 필드만 쓰고, 나머지 필드는 무시한다.

  ### 3.2 출력 데이터 (리포트)

  1. **per-document 테이블** (CSV 또는 Markdown)
     | content_id | title | orig_tok | lex_tok | vec_tok | lex_cov | flags |
     | ---------- | ----- | -------- | ------- | ------- | ------- | ----- |
  1. **요약 통계 블록** (Markdown 텍스트)

  - 샘플 수, 평균/중앙값 토큰 수
  - Lexical/Vector 토큰 감소율
  - Lexical 키워드 보존율 평균·최소
  - 이슈 패턴 요약(금액/지역 누락 등)

  ***

  ## 4. 처리 플로우

  ### 4.1 High-level

  ```
  content_summary_100_samples.json 읽기
      ↓
  각 문서(content_id 단위) 반복:
      - 원문 토큰 수 계산 (tiktoken)
      - Lexical Summary 생성 (+ 토큰 제한 적용)
      - Vector Summary 생성 (+ 토큰 제한 적용)
      - 요약 토큰 수 계산 (tiktoken)
      - Lexical 키워드 보존율 계산
      - flags 결정 (LOW_COVERAGE, TOKEN_LIMIT_VIOLATION 등)
      - 결과 row로 누적
      ↓
  전체 통계 집계
      ↓
  리포트 파일 저장 (CSV + Markdown 요약)

  ```

  ### 4.2 세부 로직

  ### 4.2.1 토큰 계산

  - 함수: `token_count_tiktoken(text: str) -> int`
  - 인코딩: `o200k_base`
  - 사용 위치:
    - `orig_tok` = `token_count_tiktoken(content_summary)`
    - `lex_tok` = Lexical Summary 최종본에 대해 동일 계산
    - `vec_tok` = Vector Summary 최종본에 대해 동일 계산

  ### 4.2.2 요약 생성

  - Lexical / Vector 모두 **ver03 요약 파이프라인** 재사용:
    - 동일 모델 / 프롬프트 / shorten 전략
    - `SUMMARY_MODELS.lexical_v1/vector_v1`의 `default_limit` 사용
  - 배치 평가에서는:
    - limit 값은 코드 상에서 고정 (프론트 입력 없음)
    - 요약 단계(step) 디버그는 필요시 로그에만 남기고, 리포트에는 최종본만 사용

  ### 4.2.3 키워드 보존율 계산

  1. 원문/lexical 요약에 대해 각각 토큰 추출:
     - 공백 기준 split
     - 1글자 토큰 제거
     - 한글/영문/숫자 조합만 남기기 (간단 필터)
     - 모두 소문자/정규화
  2. 집합 생성:

     ```
     KW_orig = set(원문 키워드)
     KW_lex  = set(lexical 요약 키워드)

     ```

  3. 보존율:

     ```
     lex_cov = len(KW_orig ∩ KW_lex) / max(1, len(KW_orig))

     ```

  4. 최소 기준:
     - `lex_cov >= 0.6` → OK
     - 그 외 → `LOW_COVERAGE` 플래그
       (도메인 슬롯 체크는 사람 리뷰용이고, 우선 자동 플래그는 coverage 기준만 사용)

  ***

  ## 5. 파이프라인 구성 (간단 아키텍처)

  ### 5.1 구성 요소

  - `loader`: JSON 파일 로더
  - `summarizer_batch`: ver03 요약 모듈 래핑 (lexical/vector 동시 호출)
  - `metrics`:
    - `token_count_tiktoken`
    - 키워드 추출 + coverage 계산
  - `reporter`:
    - per-document row 누적
    - 전체 통계 집계
    - CSV/Markdown 파일 출력

  ### 5.2 의존성

  - Python 3.x
  - tiktoken
  - 기존 ver03 요약 모듈이 사용하는 의존성 (gpt-5-mini 호출 부분은 mock 또는 실제 호출 중 택1)
    실제 배치 실행용 진짜 요약을 만들 거면:
  - gpt-5-mini 호출 그대로 사용
    테스트 전용이면:
  - 요약 부분을 mock/stub으로 바꿔서 토큰/플로우만 검증
    (이건 TDD에서 구체적으로 나눌 예정)

  ***

  ## 6. 인터페이스 설계

  ### 6.1 실행 진입점 (CLI 스크립트)

  예시:

  ```bash
  python -m app.eval.run_eval_v04 \
    --input data/content_summary_100_samples.json \
    --output-csv reports/v04_eval.csv \
    --output-md  reports/v04_eval_summary.md

  ```

  파라미터:

  - `-input`: 평가 대상 JSON 파일
  - `-output-csv`: per-document 결과 파일 경로
  - `-output-md`: 요약 리포트 (Markdown) 경로

  ### 6.2 출력 예시 (요약 리포트)

  ```markdown
  # Summary Evaluation v04 (content_summary_100_samples)

  - 샘플 수: 100
  - 평균 토큰 수:
    - 원문: 230
    - Lexical: 170
    - Vector: 130
  - 토큰 감소율:
    - Lexical: 평균 26%
    - Vector: 평균 43%
  - Lexical 키워드 보존율:
    - 평균: 0.80
    - 최소: 0.55 (6건, 수동 검토 필요)

  주요 이슈 패턴:

  - LOW_COVERAGE: 6건
  - TOKEN_LIMIT_VIOLATION: 0건
  ```

  ***

  ## 7. 체크리스트 (ver.004용)

  ### 필수

  - [ ] `token_count_tiktoken()` 기반 토큰 계산 적용
  - [ ] content_summary_100_samples.json 전체 배치 처리
  - [ ] per-document 리포트(CSV) 생성
  - [ ] 요약 통계 + 이슈 패턴 Markdown 생성
  - [ ] Lexical 키워드 보존율 계산 및 플래그링

  ### 선택(필요 시)

  - [ ] 도메인 핵심 슬롯(지역/진료과/금액) 전용 룰 추가
  - [ ] 실패 케이스 상위 N개를 별도 파일로 추출

- demo ver.005_251201
  # 📘 **기능설계서 ver.005**
  ### _LLM 기반 Lexical/Vector 요약 · 토큰/키워드 평가 · 프론트 미리보기 + 다운로드 확장_
  ***
  # 1. 목표
  **content_summary_100_samples.json**(또는 동일 스키마의 JSON 파일)을 입력받아,
  - **LLM 기반 Lexical Summary / Vector Summary 생성**
  - **토큰 감소율 평가**
  - **키워드 보존율 평가**
  - **리포트(CSV/MD) 생성 및 다운로드**
  - **LLM 실패 시 자동 재시도 + graceful fallback**
  - **프론트에서 JSON 업로드 → 요약 결과 프리뷰 + 파일 다운로드**
  까지 원스텝으로 수행하는 **완전한 오프라인 평가 + 프론트 연결 시스템**을 만든다.
  이 기능은 운영 검색엔진 도입 전
  “요약 품질 확인용 내부 QA 툴”을 만드는 것이 1차 목적이다.
  ***
  # 2. 범위
  ### 포함
  - JSON 기반 배치 평가
  - LLM(OpenAI/Google) 요약
  - Lexical/Vector limit 적용
  - token_count(tiktoken)
  - 키워드 보존율 평가
  - CSV/MD 파일 생성
  - 프론트 업로드/프리뷰/다운로드
  - LLM 재시도 및 실패 대응
  ### 제외
  - 라이브 검색엔진(`/search`) 영향 없음
  - OpenSearch 색인 없음
  - 추천 알고리즘 없음
  ***
  # 3. 입력/출력 정의
  ## 3.1 입력(JSON 스키마)
  | 필드            | 타입       | 설명        |
  | --------------- | ---------- | ----------- |
  | content_id      | int/string | 문서 식별자 |
  | title           | string     | 제목        |
  | content_summary | string     | 원문 텍스트 |
  ※ 위 3필드만 필수. 추가 필드는 무시.
  ## 3.2 출력(CSV/MD)
  ### CSV row 형식
  | content_id | title | orig_tok | lex_tok | vec_tok | lex_cov | flags |
  | ---------- | ----- | -------- | ------- | ------- | ------- | ----- |
  ### Markdown Summary
  - 샘플 수
  - 평균/중앙값 토큰 수 (orig/lex/vec)
  - Lexical token 감소율
  - Vector token 감소율
  - 키워드 보존율 평균·최소
  - LOW_COVERAGE 문서 목록
  - 실패 문서 목록(+LLM 응답 로그)
  ***
  # 4. 전체 플로우
  ```
  JSON 업로드 (프론트)
  ↓
  백엔드 /api/v1/eval/preview
      - 샘플 파싱/검증
      - 각 문서에 대해 LLM 요약 2종 생성
      - 토큰 계산
      - 키워드 보존율 계산
      - flags 생성
  ↓
  프론트 렌더링(표)
  ↓
  프론트에서 "CSV/MD 다운로드" 요청
      → 백엔드가 파일 생성 후 반환

  ```
  추가로 CLI 모드도 제공:
  ```
  python -m app.eval.run_eval_v05 \
    --input sample.json \
    --output-csv eval.csv \
    --output-md  eval.md \
    --lexical-limit 256 \
    --vector-limit 512 \
    --model openai:gpt-4.1-mini

  ```
  ***
  # 5. 핵심 로직
  # 5.1 LLM 요약 (Lexical / Vector)
  ## 공통 Wrapper
  `app/services/llm_summary.py` (신규)
  ### 인터페이스
  ```python
  async def summarize_lexical(text, limit, model, retries=3):
  async def summarize_vector(text, limit, model, retries=3):

  ```
  ### 공통 동작
  - **프롬프트 템플릿 내부에 limit 포함**
  - max_tokens는 `limit` 또는 `limit * ratio`로 설정
  - timeout = 15–20초
  - `retries`는 3회 기본 (지수 백오프 1s → 2s → 4s)
  ### 실패 처리
  - LLM timeout → retry
  - HTTP 429/500 → retry
  - 3회 실패 → `"SUMMARY_FAIL"` 문자열 반환 + flags에 `LLM_FAIL` 추가
  ***
  # 5.2 요약 프롬프트
  ### Lexical Summary
  ```
  다음 텍스트의 핵심 키워드를 최대한 유지한 요약을 생성하라.
  목표 길이: 약 {limit} 토큰 이하.

  - 금액, 지역, 시간 정보는 반드시 유지.
  - 핵심 형용사/명사도 유지.
  - 문장은 가능하면 자연스럽게 유지.

  텍스트:
  {input_text}

  ```
  ### Vector Summary
  ```
  다음 텍스트의 의미가 보존되도록 자연스럽게 재서술하라.
  목표 길이: 약 {limit} 토큰 이하.

  - 단어/표현 변경 허용.
  - 의미만 일관되게 유지.

  텍스트:
  {input_text}

  ```
  ***
  # 6. Token & Keyword Metrics
  ## 6.1 token_count_tiktoken
  - 인코딩: `o200k_base`
  - None / 빈 문자열 = 0
  ## 6.2 키워드 추출
  - 공백 split
  - 1글자 제외
  - 숫자/영문/한글만 허용
  - lower normalize
  ## 6.3 Coverage
  ```
  coverage = |orig ∩ lex| / |orig|

  ```
  ## 6.4 Flags
  - lex_cov < 0.6 → LOW_COVERAGE
  - LLM_FAIL → 요약 실패
  - TOKEN_INCREASE → 요약이 원본보다 길어진 경우
  ***
  # 7. API 설계
  ## 7.1 `/api/v1/eval/preview` (POST)
  ### Request
  ```json
  {
    "samples": [...],
    "lexical_limit": 256,
    "vector_limit": 512,
    "lexical_model": "openai:gpt-4.1-mini",
    "vector_model": "openai:gpt-4.1-mini"
  }

  ```
  ### Response
  ```json
  {
    "rows": [...],
    "summary": {...},
    "debug": { "failed": [...], "stats": {...} }
  }

  ```
  ***
  ## 7.2 `/api/v1/eval/download` (POST)
  ### 기능
  - 프론트에서 요청한 rows를 기반으로 CSV/MD 파일 생성
  - 다운로드 가능한 파일 URL 또는 base64 전달
  ### Response
  ```json
  {
    "csv": "data:text/csv;base64,...",
    "md": "data:text/markdown;base64,..."
  }
  ```
  ***
  # 8. 프론트 설계
  새 페이지 `/token-lab-llm` 생성
  ## 기능
  - JSON 업로드 → textarea에 표시
  - 모델 선택(lexical/vector)
  - limit 입력
  - “Preview” → `/eval/preview`
  - 결과 테이블 렌더링
  - “CSV 다운로드” / “MD 다운로드” 버튼 → `/eval/download`
  ## 테이블 예시
  | ID  | title | orig | lex | vec | lex_cov | flags |
  | --- | ----- | ---- | --- | --- | ------- | ----- |
  ## 디버그 패널
  - 실패 문서 리스트
  - 각 문서별 LLM 프롬프트/응답(raw) optional toggle
  ***
  # 9. 실행 모드 2종
  ## ① CLI 배치 평가
  - 대량 데이터용
  - 리포트 파일 저장
  - 운영 QA 용도
  ## ② 프론트 기반 프리뷰/다운로드
  - 빠른 실험/테스트용
  - “limit 조절 → 요약 품질 즉시 확인”
  ***
  # 10. 체크리스트 ver.004 + ver.05.1 통합
  ### LLM 요약
  - [ ] summarize_lexical 구현
  - [ ] summarize_vector 구현
  - [ ] retries / timeout / fallback 처리
  ### 평가 파이프라인
  - [ ] token_count_tiktoken 적용
  - [ ] coverage 계산
  - [ ] flags 계산
  - [ ] CSV/MD 생성
  ### API
  - [ ] /eval/preview
  - [ ] /eval/download
### 프론트
  - [ ] 파일 업로드 → 파싱
  - [ ] limit/model 입력 UI
  - [ ] Preview 결과 테이블
  - [ ] CSV/MD 다운로드 버튼

---

## demo ver.006_251201 – 통합 프리뷰/다운로드 페이지

### 목표
- 기존 token-lab(page.tsx) + app/page.tsx 기능을 하나의 페이지로 통합해, LLM/heuristic 모드 전환, 모델/limit 설정, 시스템 프롬프트 입력, 텍스트/JSON 업로드, 프리뷰/CSV·JSON 다운로드까지 한 화면에서 처리.

### 입력 UI
- 모드 선택: llm | heuristic
- LLM 모델 드롭다운: gpt-5-mini (기본), 추후 확장 가능
- Lexical/Vector 모델 드롭다운 각각 선택
- Lexical/Vector limit 숫자 입력 (토큰 제한)
- 시스템 인스트럭션 textarea (사용자 프롬프트)
- 요약 입력: 자유 텍스트 textarea 또는 JSON 파일 업로드(content_summary_*.json 스키마)

### 동작/출력
- “Preview 실행” → `/api/v1/eval/preview`
  - 요청에 mode, 모델, limit, system_prompt, samples/text 포함
  - 응답 rows를 테이블로 렌더링: content_id, title, orig_tokens, lexical_tokens, vector_tokens, keyword_cov, lexical_summary, vector_summary (orig_text는 화면 비노출)
  - debug 영역에 적용된 mode/모델/limit 표시
- “CSV 다운로드”, “JSON 다운로드” 버튼
  - `/api/v1/eval/download` 호출 후 CSV/MD 또는 CSV/JSON 다운로드 링크 제공

### 품질/제약
- limit 이하 토큰 보장: LLM 모드도 enforce_token_limit 후처리로 문장형 요약 유지
- 프롬프트 문구가 요약 결과에 섞이지 않도록 클린업
- split/truncate 금지, 자연스러운 한 문단 요약 유지 (lexical=키워드 보존, vector=의미 중심 재작성)

### 체크리스트
- [ ] 단일 페이지에 모델/limit/프롬프트/텍스트·파일 입력 UI 배치
- [ ] preview 호출 payload에 limit/model/system_prompt 포함 검증
- [ ] 테이블에 토큰/요약 컬럼 노출, orig_text는 숨김
- [ ] CSV/JSON 다운로드 버튼 동작 검증
- [ ] debug에 mode/limit/모델 표시
